{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#READING DATA SET\n",
    "\n",
    "data = pd.read_csv(\"startupdata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Couting null values in each column.\n",
    "#print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Before filling null values.\n",
    "#print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filling missing values.\n",
    "#Now filling the null values.\n",
    "def imputing_numeric_missing_values(dataset,n_neighbors=10):\n",
    "    numerical_column_names = dataset.select_dtypes([np.number]).columns\n",
    "    knn= KNNImputer()\n",
    "    knn_dataset= knn.fit_transform(dataset[numerical_column_names])\n",
    "    \n",
    "    dataset[numerical_column_names]=pd.DataFrame(knn_dataset)\n",
    "    return dataset\n",
    "\n",
    "data=imputing_numeric_missing_values(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#After fillingout null values.\n",
    "#print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing the remaining null values.\n",
    "#But before that we need to rename Unnamed: 6   to Unnamed_6  and state_code.1 to state_code_1 \n",
    "data =  data.rename(columns={\"Unnamed: 6\": \"Unnamed_6\",\"state_code.1\":\"state_code_1\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data[\"state_code_1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = data.Unnamed_6.value_counts(normalize=True)\n",
    "nan_Unnamed_6 = data['Unnamed_6'].isnull()\n",
    "data.loc[nan_Unnamed_6,'Unnamed_6'] = np.random.choice(dist.index, size=len(data[nan_Unnamed_6]),p=dist.values)\n",
    "\n",
    "dist = data.closed_at.value_counts(normalize=True)\n",
    "nan_closed_at = data['closed_at'].isnull()\n",
    "data.loc[nan_closed_at,'closed_at'] = np.random.choice(dist.index, size=len(data[nan_closed_at]),p=dist.values)\n",
    "\n",
    "dist = data.state_code_1.value_counts(normalize=True)\n",
    "nan_state_code_1 = data['state_code_1'].isnull()\n",
    "data.loc[nan_state_code_1,'state_code_1'] = np.random.choice(dist.index, size=len(data[nan_state_code_1]),p=dist.values)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#After fillingout null values.\n",
    "#data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting string values to numeric form.\n",
    "# here we convert the string based columns into integer or in numeric form.\n",
    "#Converting to feature name\n",
    "# state_code\n",
    "# zip_code\n",
    "# id\n",
    "# city\n",
    "# Unnamed_6\n",
    "# name\n",
    "# founded_at\n",
    "# closed_at\n",
    "# first_funding_at\n",
    "# last_funding_at\n",
    "# state_code_1\n",
    "# category_code\n",
    "# object_id\n",
    "# status\n",
    "\n",
    "ord_enc = OrdinalEncoder()\n",
    "enc = LabelEncoder()   \n",
    "\n",
    "enc.fit(data['state_code']) \n",
    "data[\"state_code\"] = ord_enc.fit_transform(data[[\"state_code\"]])\n",
    "\n",
    "enc.fit(data['zip_code']) \n",
    "data[\"zip_code\"] = ord_enc.fit_transform(data[[\"zip_code\"]])\n",
    "\n",
    "enc.fit(data['id']) \n",
    "data[\"id\"] = ord_enc.fit_transform(data[[\"id\"]])\n",
    "\n",
    "\n",
    "enc.fit(data['city']) \n",
    "data[\"city\"] = ord_enc.fit_transform(data[[\"city\"]])\n",
    "\n",
    "enc.fit(data['Unnamed_6']) \n",
    "data[\"Unnamed_6\"] = ord_enc.fit_transform(data[[\"Unnamed_6\"]])\n",
    "\n",
    "enc.fit(data['founded_at']) \n",
    "data[\"founded_at\"] = ord_enc.fit_transform(data[[\"founded_at\"]])\n",
    "\n",
    "enc.fit(data['closed_at']) \n",
    "data[\"closed_at\"] = ord_enc.fit_transform(data[[\"closed_at\"]])\n",
    "\n",
    "enc.fit(data['first_funding_at']) \n",
    "data[\"first_funding_at\"] = ord_enc.fit_transform(data[[\"first_funding_at\"]])\n",
    "\n",
    "enc.fit(data['last_funding_at']) \n",
    "data[\"last_funding_at\"] = ord_enc.fit_transform(data[[\"last_funding_at\"]])\n",
    "\n",
    "enc.fit(data['object_id']) \n",
    "data[\"object_id\"] = ord_enc.fit_transform(data[[\"object_id\"]])\n",
    "\n",
    "to_drop = ['state_code_1']\n",
    "data.drop(to_drop, axis=1, inplace=True)\n",
    "\n",
    "enc.fit(data['name']) \n",
    "data[\"name\"] = ord_enc.fit_transform(data[[\"name\"]])\n",
    "\n",
    "\n",
    "enc.fit(data['category_code']) \n",
    "data[\"category_code\"] = ord_enc.fit_transform(data[[\"category_code\"]])\n",
    "\n",
    "\n",
    "#Now we will convert the status to binary value.\n",
    "# 1. Binarizing the class names.\n",
    "diag_map = {'acquired':1, 'closed':0}\n",
    "data['status'] = data['status'].map(diag_map)\n",
    "# 1.1 chaging the name of colums status to : is_aquired\n",
    "data.rename(columns={'status':'is_acquired'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now standarizing some specific columns.\n",
    "#Since we have decided to keep is_acquired as out class label column. \n",
    "#for classification purposed. So except this \n",
    "#and columns which are showing labels or behaving like\n",
    "#categorical data we will not convert them.\n",
    "\n",
    "#data[0:46] = StandardScaler().fit_transform(data[0:46])\n",
    "\n",
    "# features_mean= list(data.columns[0:46])\n",
    "# data[data.columns[0:46]] = StandardScaler().fit_transform(data[data.columns[0:46]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Now making selection of attrbutes from dataset.\n",
    "#So we have two reasons to drop a column from dataset. Either that is un necessary like it may be the id of some column and \n",
    "#other reason can be that there exist some other columns which are highly co-related to it, due to that we can remove all \n",
    "#and can keep only one column.\n",
    "#So first of all we need to see that co-relation graph.\n",
    "\n",
    "# Now ploting the graph\n",
    "def draw_heatmap(dataset):\n",
    "    \n",
    "    \n",
    "    f, ax = plt.subplots(figsize = (18, 18))\n",
    "    \n",
    "    corrMatt = dataset.corr(method='spearman')\n",
    "    \n",
    "    sns.heatmap(corrMatt, annot = True, linewidth = 0.5, fmt = '.1f', ax = ax)\n",
    "    #plt.show()\n",
    "    \n",
    "    \n",
    "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    \n",
    "numerical_df_1=data.select_dtypes(numerics)\n",
    "numerical_column_names = data.select_dtypes(numerics).columns\n",
    "\n",
    "#draw_heatmap(numerical_df_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# we have summed that for strong relation threshold is 0.7>= so we have will have few columns which need to be remove due to strong relation. Following will be removed.\n",
    "\n",
    "# o\tSee there is strong co-relation between age_first_funding_year and last_funding_year so we need to remove either of them. I am removing  first_funding_year.\n",
    "\n",
    "# o\tSee there is strong co-relation between longitude and is_CA so we need to remove either of them. I am removing  longitude. \n",
    "\n",
    "# o\tSee there is strong co-relation between age_first_milestone_year and age_last_milestone_ so we need to remove either of them. I am choosing age_last_milestone_\n",
    "\n",
    "# and we will drop the all keys columns aswell.\n",
    "# 1.\tUnnamed: 0\n",
    "# 2.\tUnnamed: 6\n",
    "# 3.\tid\n",
    "# 4.\tobject_id\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "to_drop = ['age_first_funding_year','age_first_milestone_year','longitude','Unnamed: 0','Unnamed_6','id',\"object_id\"]\n",
    "data.drop(to_drop, axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Redarwing after deletion of attrbutes.\n",
    "# numerical_df_1=data.select_dtypes(numerics)\n",
    "# numerical_column_names = data.select_dtypes(numerics).columns\n",
    "\n",
    "# draw_heatmap(numerical_df_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Now we will again remove \n",
    "    # •\tlabels\n",
    "    # •\tstate_code\n",
    "    # •\tis_CA\n",
    "\n",
    "to_drop = ['labels','state_code',\"is_CA\"]\n",
    "data.drop(to_drop, axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Redarwing after deletion of attrbutes.\n",
    "# numerical_df_1=data.select_dtypes(numerics)\n",
    "# numerical_column_names = data.select_dtypes(numerics).columns\n",
    "\n",
    "# draw_heatmap(numerical_df_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generating the CSV file\n",
    "#data.to_csv ('Dataset_02_standarized_.csv', index = False, header=True)\n",
    "data.to_csv ('Dataset_02_non_standarized_.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
